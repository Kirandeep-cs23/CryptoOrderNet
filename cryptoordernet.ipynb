{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13005740,"sourceType":"datasetVersion","datasetId":8233641}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\n# List all files in the dataset folder\nbase_path = \"/kaggle/input/crypto-order-book/crypto_order_book\"\nfor root, dirs, files in os.walk(base_path):\n    print(root)\n    for f in files:\n        print(\"  \", f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:46:40.384407Z","iopub.execute_input":"2025-09-09T12:46:40.384944Z","iopub.status.idle":"2025-09-09T12:46:40.405857Z","shell.execute_reply.started":"2025-09-09T12:46:40.384918Z","shell.execute_reply":"2025-09-09T12:46:40.405217Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/crypto-order-book/crypto_order_book\n/kaggle/input/crypto-order-book/crypto_order_book/ETH\n   ETH_1min.csv\n   ETH_5min.csv\n   ETH_1sec.csv\n/kaggle/input/crypto-order-book/crypto_order_book/ADA\n   ADA_5min.csv\n   ADA_1sec.csv\n   ADA_1min.csv\n/kaggle/input/crypto-order-book/crypto_order_book/BTC\n   BTC_5min.csv\n   BTC_1sec.csv\n   BTC_1min.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\n\n# File paths\nfiles = [\n    \"/kaggle/input/crypto-order-book/crypto_order_book/ETH/ETH_1sec.csv\",\n    \"/kaggle/input/crypto-order-book/crypto_order_book/ADA/ADA_1sec.csv\",\n    \"/kaggle/input/crypto-order-book/crypto_order_book/BTC/BTC_1sec.csv\"\n]\n\n# Load and concatenate\ndfs = [pd.read_csv(f) for f in files]\ndf_all = pd.concat(dfs, ignore_index=True)\n\nprint(\"Combined DataFrame shape:\", df_all.shape)\nprint(df_all.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:46:41.969275Z","iopub.execute_input":"2025-09-09T12:46:41.970087Z","iopub.status.idle":"2025-09-09T12:48:31.086414Z","shell.execute_reply.started":"2025-09-09T12:46:41.970039Z","shell.execute_reply":"2025-09-09T12:48:31.085709Z"}},"outputs":[{"name":"stdout","text":"Combined DataFrame shape: (3092036, 156)\n   Unnamed: 0                       system_time  midpoint  spread  \\\n0           0  2021-04-07 11:32:50.861733+00:00  1970.985    0.15   \n1           1  2021-04-07 11:32:51.861733+00:00  1970.985    0.15   \n2           2  2021-04-07 11:32:52.861733+00:00  1970.985    0.15   \n3           3  2021-04-07 11:32:53.861733+00:00  1970.985    0.15   \n4           4  2021-04-07 11:32:54.861733+00:00  1971.135    0.03   \n\n          buys     sells  bids_distance_0  bids_distance_1  bids_distance_2  \\\n0     0.000000  0.000000        -0.000038        -0.000495        -0.000500   \n1     0.000000  0.000000        -0.000038        -0.000495        -0.000500   \n2     0.000000  0.000000        -0.000038        -0.000495        -0.000500   \n3     0.000000  0.000000        -0.000038        -0.000495        -0.000500   \n4  5832.007602  1.962869        -0.000008        -0.000571        -0.000576   \n\n   bids_distance_3  ...  asks_market_notional_5  asks_market_notional_6  \\\n0        -0.000632  ...                     0.0                     0.0   \n1        -0.000632  ...                     0.0                     0.0   \n2        -0.000632  ...                     0.0                     0.0   \n3        -0.000632  ...                     0.0                     0.0   \n4        -0.000601  ...                     0.0                     0.0   \n\n   asks_market_notional_7  asks_market_notional_8  asks_market_notional_9  \\\n0                     0.0                     0.0                     0.0   \n1                     0.0                     0.0                     0.0   \n2                     0.0                     0.0                     0.0   \n3                     0.0                     0.0                     0.0   \n4                     0.0                     0.0                     0.0   \n\n   asks_market_notional_10  asks_market_notional_11  asks_market_notional_12  \\\n0                      0.0                      0.0                      0.0   \n1                      0.0                      0.0                      0.0   \n2                      0.0                      0.0                      0.0   \n3                      0.0                      0.0                      0.0   \n4                      0.0                      0.0                      0.0   \n\n   asks_market_notional_13  asks_market_notional_14  \n0                      0.0                      0.0  \n1                      0.0                      0.0  \n2                      0.0                      0.0  \n3                      0.0                      0.0  \n4                      0.0                      0.0  \n\n[5 rows x 156 columns]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Check columns of your combined dataset\nprint(df_all.columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:51:19.696344Z","iopub.execute_input":"2025-09-09T12:51:19.697099Z","iopub.status.idle":"2025-09-09T12:51:19.702478Z","shell.execute_reply.started":"2025-09-09T12:51:19.697071Z","shell.execute_reply":"2025-09-09T12:51:19.701905Z"}},"outputs":[{"name":"stdout","text":"Index(['Unnamed: 0', 'system_time', 'midpoint', 'spread', 'buys', 'sells',\n       'bids_distance_0', 'bids_distance_1', 'bids_distance_2',\n       'bids_distance_3',\n       ...\n       'asks_market_notional_5', 'asks_market_notional_6',\n       'asks_market_notional_7', 'asks_market_notional_8',\n       'asks_market_notional_9', 'asks_market_notional_10',\n       'asks_market_notional_11', 'asks_market_notional_12',\n       'asks_market_notional_13', 'asks_market_notional_14'],\n      dtype='object', length=156)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import numpy as np\n\nSEQ_LEN = 50  # sequence length\n\n# Select a manageable set of features (you had 156 total, but let's use fewer first)\nfeature_cols = [\n    \"midpoint\", \"spread\", \"buys\", \"sells\",\n    \"bids_distance_0\", \"bids_distance_1\", \"bids_distance_2\",\n    \"asks_distance_0\", \"asks_distance_1\", \"asks_distance_2\"\n]\ndf_features = df_all[feature_cols].fillna(0).values\n\n# Target: future mid-price movement\ndf_target = np.sign(df_all[\"midpoint\"].shift(-1) - df_all[\"midpoint\"])\ndf_target = (df_target > 0).astype(int).values  # 1 = up, 0 = down/flat\n\n# Build sequences\nX_sequences, y_sequences = [], []\nfor i in range(len(df_features) - SEQ_LEN):\n    X_sequences.append(df_features[i:i+SEQ_LEN])\n    y_sequences.append(df_target[i+SEQ_LEN])\n\nX_sequences = np.array(X_sequences)\ny_sequences = np.array(y_sequences)\n\nprint(\"Sequences shape:\", X_sequences.shape, \"Labels shape:\", y_sequences.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:51:24.175457Z","iopub.execute_input":"2025-09-09T12:51:24.176141Z","iopub.status.idle":"2025-09-09T12:51:34.926673Z","shell.execute_reply.started":"2025-09-09T12:51:24.176114Z","shell.execute_reply":"2025-09-09T12:51:34.926061Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in sign\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Sequences shape: (3091986, 50, 10) Labels shape: (3091986,)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# ======================\n# 1. Sample data (to fit in RAM/GPU)\"\"\n# ======================\nN_SAMPLES = 200_000   # keep this safe for Kaggle P100, increase later\nidx = np.random.choice(len(X_sequences), size=N_SAMPLES, replace=False)\nX_sample = X_sequences[idx]\ny_sample = y_sequences[idx]\n\n# ======================\n# 2. Normalize features\n# ======================\nscaler = StandardScaler()\nX_reshaped = X_sample.reshape(-1, X_sample.shape[2])  # (N*seq_len, features)\nX_scaled = scaler.fit_transform(X_reshaped)\nX_sample = X_scaled.reshape(X_sample.shape)  # back to (N, seq_len, features)\n\n# ======================\n# 3. Train/val split\n# ======================\nsplit = int(0.8 * len(X_sample))\nX_train, X_val = X_sample[:split], X_sample[split:]\ny_train, y_val = y_sample[:split], y_sample[split:]\n\n# Convert to torch\nX_train_t = torch.tensor(X_train, dtype=torch.float32)\ny_train_t = torch.tensor(y_train, dtype=torch.long)\nX_val_t   = torch.tensor(X_val, dtype=torch.float32)\ny_val_t   = torch.tensor(y_val, dtype=torch.long)\n\ntrain_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=64, shuffle=True)\nval_loader   = DataLoader(TensorDataset(X_val_t, y_val_t), batch_size=64)\n\n# ======================\n# 4. Model (TCN + Attention)\n# ======================\nclass Chomp1d(nn.Module):\n    def __init__(self, chomp_size):\n        super().__init__()\n        self.chomp_size = chomp_size\n    def forward(self, x):\n        return x[:, :, :-self.chomp_size].contiguous()\n\nclass TemporalBlock(nn.Module):\n    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n        super().__init__()\n        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size, stride=stride,\n                               padding=padding, dilation=dilation)\n        self.chomp1 = Chomp1d(padding)\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(dropout)\n\n        self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size, stride=stride,\n                               padding=padding, dilation=dilation)\n        self.chomp2 = Chomp1d(padding)\n        self.relu2 = nn.ReLU()\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        out = self.net(x)\n        res = x if self.downsample is None else self.downsample(x)\n        return self.relu(out + res)\n\nclass TemporalConvNet(nn.Module):\n    def __init__(self, num_inputs, num_channels, kernel_size=3, dropout=0.2):\n        super().__init__()\n        layers = []\n        for i in range(len(num_channels)):\n            dilation_size = 2 ** i\n            in_ch = num_inputs if i == 0 else num_channels[i-1]\n            out_ch = num_channels[i]\n            layers.append(TemporalBlock(in_ch, out_ch, kernel_size, stride=1,\n                                        dilation=dilation_size, padding=(kernel_size-1)*dilation_size,\n                                        dropout=dropout))\n        self.network = nn.Sequential(*layers)\n    def forward(self, x):\n        return self.network(x)\n\nclass TCN_Attention(nn.Module):\n    def __init__(self, num_features, num_classes=2):\n        super().__init__()\n        self.tcn = TemporalConvNet(num_features, [64, 64, 64], kernel_size=3, dropout=0.2)\n        self.attention = nn.Sequential(\n            nn.Linear(64, 32),\n            nn.Tanh(),\n            nn.Linear(32, 1)\n        )\n        self.fc = nn.Linear(64, num_classes)\n    def forward(self, x):\n        x = x.transpose(1, 2)              # (batch, features, seq_len)\n        tcn_out = self.tcn(x)              # (batch, channels, seq_len)\n        tcn_out = tcn_out.transpose(1, 2)  # (batch, seq_len, channels)\n        attn_weights = torch.softmax(self.attention(tcn_out), dim=1)  # (batch, seq_len, 1)\n        context = torch.sum(attn_weights * tcn_out, dim=1)            # (batch, channels)\n        return self.fc(context)\n\n# ======================\n# 5. Training\n# ======================\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = TCN_Attention(num_features=X_train.shape[2]).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nEPOCHS = 20\nfor epoch in range(EPOCHS):\n    model.train()\n    total_loss = 0\n    for xb, yb in train_loader:\n        xb, yb = xb.to(device), yb.to(device)\n        optimizer.zero_grad()\n        out = model(xb)\n        loss = criterion(out, yb)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    avg_loss = total_loss / len(train_loader)\n\n    # Validation\n    model.eval()\n    val_preds, val_labels = [], []\n    with torch.no_grad():\n        for xb, yb in val_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            out = model(xb)\n            preds = torch.argmax(out, dim=1)\n            val_preds.extend(preds.cpu().numpy())\n            val_labels.extend(yb.cpu().numpy())\n    val_acc = accuracy_score(val_labels, val_preds)\n    val_f1 = f1_score(val_labels, val_preds, average='macro')\n    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T10:03:26.682749Z","iopub.execute_input":"2025-09-09T10:03:26.683073Z","iopub.status.idle":"2025-09-09T10:11:01.854373Z","shell.execute_reply.started":"2025-09-09T10:03:26.683048Z","shell.execute_reply":"2025-09-09T10:11:01.853569Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20, Loss: 0.5783, Val Acc: 0.7191, Val F1: 0.4188\nEpoch 2/20, Loss: 0.5743, Val Acc: 0.7191, Val F1: 0.4196\nEpoch 3/20, Loss: 0.5728, Val Acc: 0.7191, Val F1: 0.4184\nEpoch 4/20, Loss: 0.5718, Val Acc: 0.7188, Val F1: 0.4210\nEpoch 5/20, Loss: 0.5711, Val Acc: 0.7182, Val F1: 0.4240\nEpoch 6/20, Loss: 0.5707, Val Acc: 0.7185, Val F1: 0.4206\nEpoch 7/20, Loss: 0.5700, Val Acc: 0.7190, Val F1: 0.4198\nEpoch 8/20, Loss: 0.5692, Val Acc: 0.7191, Val F1: 0.4191\nEpoch 9/20, Loss: 0.5688, Val Acc: 0.7192, Val F1: 0.4195\nEpoch 10/20, Loss: 0.5686, Val Acc: 0.7182, Val F1: 0.4257\nEpoch 11/20, Loss: 0.5680, Val Acc: 0.7186, Val F1: 0.4201\nEpoch 12/20, Loss: 0.5678, Val Acc: 0.7188, Val F1: 0.4206\nEpoch 13/20, Loss: 0.5676, Val Acc: 0.7183, Val F1: 0.4233\nEpoch 14/20, Loss: 0.5670, Val Acc: 0.7175, Val F1: 0.4240\nEpoch 15/20, Loss: 0.5669, Val Acc: 0.7189, Val F1: 0.4214\nEpoch 16/20, Loss: 0.5667, Val Acc: 0.7183, Val F1: 0.4253\nEpoch 17/20, Loss: 0.5663, Val Acc: 0.7186, Val F1: 0.4216\nEpoch 18/20, Loss: 0.5663, Val Acc: 0.7190, Val F1: 0.4210\nEpoch 19/20, Loss: 0.5661, Val Acc: 0.7190, Val F1: 0.4188\nEpoch 20/20, Loss: 0.5661, Val Acc: 0.7183, Val F1: 0.4248\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import numpy as np\n\nsequence_length = 50\nsequences = []\nlabels = []\n\nfor i in range(len(X) - sequence_length):\n    sequences.append(X[i:i+sequence_length])\n    labels.append(y[i+sequence_length])\n\nX_seq = np.array(sequences)\ny_seq = np.array(labels)\n\nprint(\"Sequences shape:\", X_seq.shape, \"Labels shape:\", y_seq.shape)\n# Output: (num_samples, sequence_length, num_features)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T08:17:58.413865Z","iopub.status.idle":"2025-09-09T08:17:58.414171Z","shell.execute_reply.started":"2025-09-09T08:17:58.414014Z","shell.execute_reply":"2025-09-09T08:17:58.414029Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================================\n# Low-Latency Microstructure Prediction Engine\n# Dataset: Crypto Limit Order Book (LOB) Example\n# Model: TCN + Attention\n# ===============================================\n\n# -----------------------------\n# 1. Imports\n# -----------------------------\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# -----------------------------\n# 2. Dataset Loader & Feature Engineering\n# -----------------------------\nclass LOBDataset(Dataset):\n    def __init__(self, df, sequence_length=50):\n        self.sequence_length = sequence_length\n        self.features, self.labels = self.preprocess(df)\n    \n    def preprocess(self, df):\n        # Feature Engineering\n        # Mid-price\n        df['mid_price'] = (df['bid_1'] + df['ask_1']) / 2\n        df['mid_price_shift'] = df['mid_price'].shift(-1)\n        # Label: 1 = up, 0 = down, 2 = neutral\n        df['label'] = np.where(df['mid_price_shift'] > df['mid_price'], 1,\n                               np.where(df['mid_price_shift'] < df['mid_price'], 0, 2))\n        \n        # Select features\n        feature_cols = ['bid_1','ask_1','bid_2','ask_2','bid_3','ask_3',\n                        'bid_1_size','ask_1_size','bid_2_size','ask_2_size',\n                        'bid_3_size','ask_3_size','mid_price']\n        X = df[feature_cols].fillna(0).values\n        y = df['label'].fillna(2).astype(int).values\n        \n        # Normalize features\n        scaler = StandardScaler()\n        X = scaler.fit_transform(X)\n        \n        # Create sequences\n        sequences = []\n        seq_labels = []\n        for i in range(len(X) - self.sequence_length):\n            sequences.append(X[i:i+self.sequence_length])\n            seq_labels.append(y[i+self.sequence_length])\n        return torch.tensor(np.array(sequences), dtype=torch.float32), torch.tensor(seq_labels, dtype=torch.long)\n    \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, idx):\n        return self.features[idx], self.labels[idx]\n\n# -----------------------------\n# 3. Model: TCN + Attention\n# -----------------------------\nclass Chomp1d(nn.Module):\n    def __init__(self, chomp_size):\n        super().__init__()\n        self.chomp_size = chomp_size\n    def forward(self, x):\n        return x[:, :, :-self.chomp_size].contiguous()\n\nclass TemporalBlock(nn.Module):\n    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n        super().__init__()\n        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,\n                               stride=stride, padding=padding, dilation=dilation)\n        self.chomp1 = Chomp1d(padding)\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(dropout)\n        \n        self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size,\n                               stride=stride, padding=padding, dilation=dilation)\n        self.chomp2 = Chomp1d(padding)\n        self.relu2 = nn.ReLU()\n        self.dropout2 = nn.Dropout(dropout)\n        \n        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n        self.relu = nn.ReLU()\n    \n    def forward(self, x):\n        out = self.net(x)\n        res = x if self.downsample is None else self.downsample(x)\n        return self.relu(out + res)\n\nclass TCN(nn.Module):\n    def __init__(self, input_size, output_size, num_channels, kernel_size=3, dropout=0.2):\n        super().__init__()\n        layers = []\n        num_levels = len(num_channels)\n        for i in range(num_levels):\n            dilation_size = 2 ** i\n            in_channels = input_size if i == 0 else num_channels[i-1]\n            out_channels = num_channels[i]\n            layers.append(TemporalBlock(in_channels, out_channels, kernel_size, stride=1,\n                                        dilation=dilation_size, padding=(kernel_size-1)*dilation_size,\n                                        dropout=dropout))\n        self.tcn = nn.Sequential(*layers)\n        self.attention = nn.Sequential(\n            nn.Linear(num_channels[-1], 64),\n            nn.Tanh(),\n            nn.Linear(64, 1),\n            nn.Softmax(dim=1)\n        )\n        self.fc = nn.Linear(num_channels[-1], output_size)\n    \n    def forward(self, x):\n        # x shape: batch, seq_len, features -> TCN expects batch, features, seq_len\n        x = x.transpose(1,2)\n        y1 = self.tcn(x)  # batch, channels, seq_len\n        y1 = y1.transpose(1,2)  # batch, seq_len, channels\n        attn_weights = self.attention(y1)  # batch, seq_len, 1\n        y = torch.sum(attn_weights * y1, dim=1)  # weighted sum over sequence\n        out = self.fc(y)\n        return out\n\n# -----------------------------\n# 4. Load Dataset\n# -----------------------------\n# Example: Using a synthetic CSV (replace with real LOB data)\n# CSV should have columns: bid_1, ask_1, bid_2, ask_2, bid_3, ask_3, bid_1_size, ask_1_size, ...\ndf = pd.read_csv(\"/kaggle/input/crypto-lob-sample.csv\")\ntrain_df, val_df = train_test_split(df, test_size=0.2, shuffle=False)\n\ntrain_dataset = LOBDataset(train_df)\nval_dataset = LOBDataset(val_df)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n\n# -----------------------------\n# 5. Train the Model\n# -----------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = TCN(input_size=train_dataset.features.shape[2],\n            output_size=3,\n            num_channels=[64,64,64,64]).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0\n    for X_batch, y_batch in train_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        optimizer.zero_grad()\n        outputs = model(X_batch)\n        loss = criterion(outputs, y_batch)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss/len(train_loader):.4f}\")\n\n# -----------------------------\n# 6. Evaluate\n# -----------------------------\nmodel.eval()\nall_preds, all_labels = [], []\nwith torch.no_grad():\n    for X_batch, y_batch in val_loader:\n        X_batch = X_batch.to(device)\n        outputs = model(X_batch)\n        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n        all_preds.extend(preds)\n        all_labels.extend(y_batch.numpy())\n\nacc = accuracy_score(all_labels, all_preds)\nf1 = f1_score(all_labels, all_preds, average='weighted')\nprint(f\"Validation Accuracy: {acc:.4f}, F1 Score: {f1:.4f}\")\n\n# -----------------------------\n# 7. Optional: TorchScript Export for Low-Latency\n# -----------------------------\nscripted_model = torch.jit.script(model)\nscripted_model.save(\"tcn_attn_model.pt\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-09T07:59:14.531047Z","iopub.execute_input":"2025-09-09T07:59:14.531326Z","iopub.status.idle":"2025-09-09T07:59:19.737765Z","shell.execute_reply.started":"2025-09-09T07:59:14.531302Z","shell.execute_reply":"2025-09-09T07:59:19.736458Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3358718097.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;31m# Example: Using a synthetic CSV (replace with real LOB data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;31m# CSV should have columns: bid_1, ask_1, bid_2, ask_2, bid_3, ask_3, bid_1_size, ask_1_size, ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/crypto-lob-sample.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/crypto-lob-sample.csv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/crypto-lob-sample.csv'","output_type":"error"}],"execution_count":1}]}